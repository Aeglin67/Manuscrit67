\chapter[Contribution à l'algorithme FCM]{Contribution à l'algorithme FCM et application à la segmentation cérébrale}
\label{chap:nlfcm}
\minitoc

Dans ce chapitre, une nouvelle méthode de segmentation basée sur FCM est définie.
Elle inclue l'approche des moyennes non-locales (en anglais, \emph{non-local means}) proposée en 2005.
Issue de travaux dans le cadre du débruitage d'images, cette dernière a été spécifiée de manière indépendante par \cite{Buades:MMS:2005}, et \cite{Awate:PAMI:2006}.
L'idée générale est de profiter de la redondance de l'information au sein d'une image pour estimer la \og vrai \fg{} valeur en chaque voxel.
La nécessité d'avoir une évaluation robuste de ces similarités entre voxels a conduit à prendre en compte leur proche voisinage, conduisant à la définition de patches.

L'utilisation de la redondance de l'information de l'image est une option peu explorée en segmentation cérébrale.
Comme nous l'avons vu précédemment, de nombreuses méthodes utilisent une modélisation spécifique d'un champ de biais pour la correction de l'inhomogénéité de l'intensité ou une approche prenant en compte la classification des voxels environnants pour la prise en compte du bruit.
Certaines méthodologies de types FCM ont inclu une prise en compte de la redondance de l'information en favorisant les voxels dont l'intensité et les coordonées spatiales sont les plus proches de celles du voxel courant (voir en Section~\ref{subsubsec:fcm:ponderation}).
Cependant, les travaux sur les méthodes de débruitage par patches (dont l'approche non-locale) ont montré que ces dernières étaient efficaces car elles offrent une estimation de la similarité plus fiable qu'une comparaison de la seule valeur des voxels~\cite{Salmon:2010}.
Un terme de régularisation non-local intègre don une meilleure prise en compte de l'environnement autour du voxel courant pour effectuer la régularisation.

Concernant la prise en compte du biais, une méthodologie de type FCM a introduit une évaluation locale des centroïdes afin de ne pas avoir à évaluer le champ de biais directement (voir en Section~\ref{subsec:fcm:bias}).
Cependant, la taille du voisinage utilisé pour estimer ces centroïdes locaux est difficile à fixer et peut amener une mauvaise estimation des centroïdes locaux, biaisant ainsi la classification des voxels.
L'introduction des moyennes non-locales va donc permettre de prendre en compte l'estimation des centroïdes locaux voisins tout en introduisant une pondération appropriée et de rendre le processus de segmentation globalement plus robuste.

La suite du chapitre s'organise de la façon suivante.
Dans un premier temps, les moyennes non-locales sont définies et une synthèse des travaux les utilisant est présentée.
Par la suite, l'intégration de cette approche dans l'algorithme FCM est discutée en distinguant l'apport au terme d'attache aux données de celui au terme de régularisation.
Enfin, des validations, ainsi qu'une évaluation des performances de l'algorithme proposé par rapport à d'autres méthodes (inspirées de FCM ou fondées sur les champs et chaînes de Markov) sont réalisées.

\section{L'approche non-locale}
\label{sec:nl:def}

\subsection{Définition}

L'image $I$ est considérée comme étant une fonction $I : \Omega \rightarrow Y$, où $\Omega$ est le support de l'image contenant $N$ voxels et $Y$ est l'espace des intensités.
Le débruitage par moyennes non-locales et l'évaluation des redondances reposent sur l'utilisation de patches, qui sont définis comme un voisinage cubique autour d'un voxel d'intérêt $j$, de rayon $W$ et indexé par son voxel central (voir la figure~\ref{fig:mean_filter}).
Un patch est noté de la manière suivante : 
\begin{equation}
P_{j} = P^{W}_{j} = \left( I(\mathbf{x}_j+\mathbf{\tau}), \mathbf{\tau} \in [\![ -W, W ]\!]^{d} \right) \label{nlfcm:patch:def},
\end{equation}
où $d$ est la dimension de l'image $I$ et $\mathbf{x}_j$ est le vecteur correspondant aux coordonnées spatiales du voxel $j$.

L'estimation non-locale de l'intensité corrigée au voxel $j$ est donnée par : 
\begin{equation}
\mathbf{\hat{y}}_{j} = \frac{\sum_{j'=1}^{N} K \left( \frac{\lVert P^{I}_{j} - P^{I}_{j'} \rVert}{h} \right) \cdotp \mathbf{y}_{j} }{\sum_{j'=1}^{N} K \left(\frac{ \lVert P^{I}_{j} - P^{I}_{j'} \rVert}{h} \right)} \label{nlfcm:estimateur:complet},
\end{equation}
où $K(\cdotp)$ est un noyau, c'est-à-dire une fonction de pondération de $\mathbb{R}$ dans $\mathbb{R}$ et $h$ est la fenêtre régissant le degré de lissage de l'estimation de la similarité.
Le calcul de cette dernière est effectué par la comparaison entre les patches respectifs des voxels considérés et est contrôlé par le choix de la norme $\lVert \cdotp \rVert$.
Cette formulation correspond à une moyenne pondérée des voxels sur l'ensemble du support de l'image.

Cela nous permet de réécrire la formule \ref{nlfcm:estimateur:complet} sous la forme :
\begin{equation}
\mathbf{\hat{y}}_{j} = \sum_{j' = 1}^{N}\omega_{nl}(j,j') \cdotp \mathbf{y}_{j'} \label{nlfcm:estimateur:court},
\end{equation}
où $\omega_{nl}(j,j')$ est le poids non-local attribué au voxel $j'$ pour l'estimation de $\mathbf{\hat{y}}_{j}$. 
Ce terme est défini par :
\begin{equation}
\omega_{nl}(j,j') = \frac{K \left( \frac{\lVert P^{I}_{j} - P^{I}_{j'} \rVert}{h} \right)}{\sum_{i=1}^{N} K \left(\frac{ \lVert P^{I}_{j} - P^{I}_{i} \rVert}{h} \right)} \label{nlfcm:poids:definition},
\end{equation}
avec $\sum_{j'=1}^{N} \omega_{nl}(j,j') = 1$.
Les paramètres régissant le comportement des moyennes non-locales sont donc : la fenêtre $h$, la taille et la forme du patch, la norme utilisée pour le calcul de la différence entre les patches et le noyau $K$.
Un paramètre supplémentaire vient s'ajouter à la liste ci-dessus.
Pour des raisons pratiques le calcul des poids non-locaux a été restreint à une zone de recherche $\Omega_{j}^{R}$ autour du voxel $j$.
L'article fondateur de \cite{Buades:MMS:2005} propose déjà cette solution.

\begin{figure}[!t]
\begin{center}
\subfigure[]{\includegraphics[height=55mm]{eps/chapitre3/Mean_Filter.eps}}
\subfigure[]{\includegraphics[height=55mm]{eps/chapitre3/NL_Mean_Filter.eps}}
\end{center}
\caption[Comparaison entre un débruitage par moyennes et par moyennes non-locales]{\emph{(a) Débruitage par moyennes. L'estimation de l'intensité du voxel central est une moyenne de l'ensemble des voxels du voisinage $N_j$. (b) Débruitage par moyennes non-locales~\cite{Buades:MMS:2005}. Un poids est attribué à chaque voxel de la zone de recherche $\Omega^R_j$ selon la similarité de son patch avec celui du voxel central.}}
\label{fig:mean_filter}
\end{figure}

Telle qu'elles sont définies, les moyennes non-locales ont montrés une grande efficacité dans le débruitage de zones régulières mais aussi dans des zones texturées~\cite{Salmon:2010}, ce qui est peu étonnant vu que certains travaux préliminaires ayant mené à cette approche ont été fait dans le cadre de la synthèse de textures~\cite{Criminisi:CVPR:2003}.
% 

% A rajouter dans l'état de l'art : \cite{VanDeVille:TIP:2011}.

\subsection{Interprétation des moyennes non-locales}

Plusieurs points de vue peuvent être adoptés pour l'interprétation des moyennes-non-locales.
Un premier lien a été établi avec les variations totales, où le débruitage d'une image consiste en la minimisation d'une fonctionnelle constituée de la somme d'un terme d'attache aux données (différence entre l'image estimée et l'image originale) et d'un terme de régularisation, pénalisant le bruit à haute-fréquence.
Des termes non-locaux sont introduits dans cette fonctionnelle afin de favoriser les similarité de l'image \cite{Kindermann:MMS:2005, Gilboa:MMS:2008, Azzabou:CVPR:2007}.
Dans ce cadre, les moyennes non-locales sont vu comme une étape d'une descente de gradient pour la minimisation de la fonctionelle .

Les moyennes non-locales ont également été étudiées directement depuis l'espace des patches.
Les travaux de \cite{Tschumperle:ICIP:2009} les interprètent comme une étape d'une équation de chaleur dans l'espace des patches et établissent des liens avec les algorithmes classiques de diffusion.
Ce point de vue est soutenu par l'article de \cite{Peyre:CVIU:2009} qui considère qu'une image repose sur une variété dans l'espace des patches, les moyennes non-locales consistant en une diffusion dans cette variété.
Enfin, un lien avec les équations de Fokker-Planck est mis en évidence par \cite{Singer:SIAMJIS:2009}, rapprochant les moyennes non-locales des processus de Markov.

D'autres auteurs adoptent un point de vue statistiques pour interpréter les moyennes non-locales.
Par exemple, \cite{Goossens:LNLA:2008} démontre un lien avec les estimateurs robustes (ou plutôt, la première itération d'un algorithme d'optimisation) et pose la question de l'utilisation du noyau gaussien si le bruit ne l'est pas.
Par ailleurs, \cite{Deledalle:TIP:2009} propose un algorithme itératif fondé sur l'estimation du maximum de vraisemblance permettant de prendre en compte différents types de bruit.
Enfin, \cite{Salmon:ICIP:2009} considère les moyennes non-locales comme une agrégation bayésienne d'estimateurs en chaque patches.
L'article de \cite{Katkovnik:IJCV:2010} assimile quant à lui la première version des moyennes non-locales à une régression linéaire d'ordre $0$ et l'étend à des niveaux supérieurs.

De plus, certains travaux ont permis d'établir un lien entre le filtre non-local et les différents filtres déjà existants (tel que le filtre bilatéral, les filtres bayésiens,\ldots).
Des techniques de régularisation de graphes discrètes, (voir les travaux de \cite{Bougleux:IJCV:2009} et \cite{Elmoataz:TIP:2008}) permettent de faire le lien entre ces différentes techniques.
Ces méthodes de régularisation peuvent être vues comme des versions discrètes d'un formalisme plus général appellé \emph{Non local data and smoothness term (NDS)} \cite{Mrazek:2006}. 
Ce formalisme définit une forme générale permettant l'unification de nombreux filtres existants par la somme d'un terme d'attache aux données et d'un terme de lissage.
Il est étendu à la prise en compte des patches par l'article de \cite{Pizarro:IJCV:2010}, définissant ainsi le \emph{Generalized NDS (GNDS)}, dont le NDS est un cas particulier.


\subsection{Limites de l'approche non-locale et influence de ses paramètres}

Les deux inovations permettant une grande efficacité de ce filtre sont la non-localité et l'utilisation de patches.
Deux hypothèses importantes~\cite{Szlam:UCLA:2008} sont ainsi faites concernant l'image traitée : 
\begin{enumerate}
        \item il existe des patches similaires dans l'image (hypothèse de redondance de l'information),
        \item des patches similaires ont des voxels centraux similaires.
\end{enumerate}

Il est cependant démontré \cite{Duval:2011} que ces hypothèses ont plusieurs conséquences lors du débruitage des images : 
\begin{itemize}
 \item le filtre a un impact conséquent sur des images périodiques (par exemple des images alternant des bandes blanches et noires)
 \item la zone de recherche $\Omega^R_j$ a un impact sur la qualité visuelle du débruitage,
 \item un grand patch tend à flouter l'image,
 \item une perte de contraste dépendant du niveau d'occurence de chaque patch est observée,
 \item moins les détails sont contrastés, plus ils sont dégradés.
\end{itemize}
Le choix des paramètres se fera donc en tenant compte de ces propriétés.

Plusieurs stratégies ont été adoptées pour calculer de manière automatique le paramètre $h$ \cite{Tasdizen:ICIP:2008,Coupe:TMI:2008}. 
L'article de \cite{Buades:MMS:2005} propose de fixer ce paramètre selon la règle : $h = 10\sigma$, où $\sigma$ est l'écart-type du bruit.
Cependant, les travaux de \cite{Coupe:TMI:2008} proposent une autre méthode de calcul en montrant que $h$ ne dépend  pas que de la variance du bruit $\sigma^2$, mais également de la taille de la zone de recherche $\lvert \Omega^{R}_{j} \rvert$.
La fenêtre $h$ peut ainsi être évaluée par : $h = 2 \alpha \sigma^{2} \lvert \Omega^{R}_{j} \rvert$ où seul le paramètre $\alpha$ est à ajuster manuellement.
Dans la cas d'un bruit gaussien, la valeur de $\alpha$ est théoriquement à $1$ si l'estimation de la variance est correcte.
Un dernier exemple d'ajustement est le travail de \cite{Tasdizen:TIP:2009} qui réalise un apprentissage sur différentes images ayant un niveau de bruit connu.
La relation linéaire entre $h$ et $\sigma$ est mise en évidence et ses paramètres calculés en fonction du niveau de bruit et du paramètre $h$ permettant un débruitage optimal.

La zone de recherche $\Omega^R_j$ a été introduite d'abord dans un soucis d'amélioration du temps de calcul. 
Cependant, certains auteurs ont observé qu'elle avait une influence déterminante sur la qualité visuelle du débruitage \cite{Kervrann:TIP:2006,Gilboa:MMS:2007}.
En effet, l'utilisation d'une zone de recherche trop grande peut conduire à une baisse des performances alors qu'intuitivement, une zone de recherche de plus en plus grande devrait augmenter le nombre de \og bons \fg{} candidats pour effectuer une estimation fiable de la valeur en chaque voxel. 
En pratique, dans les zones où la frontière entre les objets est bien marquée, peu de candidats supplémentaires sont ajoutés tandis que de nombreux \og mauvais \fg{} le sont, conduisant finalement à une moins bonne estimation.
Cette zone de recherche est donc à ajuster en fonction de l'image considérée \cite{Salmon:2010}. 
Elle peut également être déterminée localement selon la règle de Lepski en fonction du niveau de bruit local, comme dans l'article de \cite{Kervrann:TIP:2006}.

\begin{figure}[!t]
\begin{center}
\includegraphics[height=55mm]{eps/chapitre3/kernels.eps}
\end{center}
\caption[Différents noyaux utilisés dans le cadre des moyennes non-locales]{\emph{Différents noyaux utilisés dans le cadre des moyennes non-locales. Noir : noyau plat. Vert : noyau gaussien $K(x) = \exp{(-x^2)}$. Rouge : noyau compact polynomial de degré 4. Bleu : noyau compact polynomial de degré 6.}}
\label{fig:nlm:kernels}
\end{figure}

D'autres articles ont étudié l'impact de la fonction noyau sur le débruitage. 
L'article de Buades \cite{Buades:MMS:2005} utilise un noyau de type gaussien $K(x) = e^{-x^2}$, qui est encore largement utilisé dans la littérature.
Cependant, des travaux récents \cite{Goossens:LNLA:2008, Salmon:2010} ont montré qu'un noyau à support compact \cite{Remaki:TIP:2000} est préférable.
En effet, le noyau gaussien a l'inconvénient de donner une valeur proche de zéro, mais non nulle, aux patches peu similaires à celui entourant le voxel central, ce qui tend à diminuer l'efficacité du débruitage. 
Certains travaux tronquent le noyau gaussien pour éviter ce problème et pour accélérer les traitements \cite{Coupe:TMI:2008}.
Cependant, l'utilisation de noyaux compacts apporte une réponse plus naturelle à ce problème en définissant l'influence des patches peu similaires au patch central comme nulle, tout en assurant la continuité de ces fonctionnelles. 
Un noyau plat peut être employé afin de privilégier la vitesse de calcul (mais n'est pas continu) et on peut trouver des noyaux compacts polynomiaux de degré $4$ \cite{Goossens:LNLA:2008} ou de degré $6$ \cite{Duval:2010}.
La figure~\ref{fig:nlm:kernels} illustrent cette problématique en comparant différents noyaux et démontre l'utilité d'un noyau plat par rapport au noyau gaussien (en ne prenant en compte que les patches \og proches \fg{} du patch central) et par rapport au noyau plat (qui n'est pas une fonction continue et ne pondère pas les patches).

Le choix de la norme est aussi un élément à prendre en compte.
De manière générale, l'utilisation la norme euclidienne pour le calcul de la similarité entre deux patches a peu été remis en cause \cite{Salmon:2010}.
Cependant, quelques alternatives au calcul direct de la distance entre les patches peuvent être notés.
L'article de \cite{Buades:MMS:2005} inclut également une pondération en fonction de la distance des voxels par rapport au voxel central.
Cependant, il semble que cette pondération ait été peu retenue dans les travaux ultérieurs.
Les travaux de \cite{Azzabou:ICIP:2007} et \cite{Tasdizen:TIP:2009} utilisent un dictionnaire issu de l'image et calculé par une analyse des composantes principales (ACP) de la matrice de covariance construite à partir de l'ensemble des voisinages de l'image.
La comparaison des patches se fait alors dans l'espace construit à partir des composantes présentant les valeurs propres les plus élevées. 
Les résultats montrent une nette amélioration des performances du débruitage.

L'influence de la taille des patches a été étudiée par \cite{Mairal:ECCV:2009}.
Le niveau du bruit est à prendre en compte pour déterminer la taille et/ou la forme de ce paramètre.
Plus le bruit est important, plus il est nécessaire d'utiliser des grands patches afin de réaliser une estimation robuste de la similarité.
De plus, le bruit peut ne pas être uniforme sur l'image et la taille du patch doit donc être idéalement définie localement.
Enfin, de larges patches sont peu adaptés dans les zones présentant de fortes variations , ayant pour conséquence un effacement des détails isolés et la présence d'un halo de bruit autour de ces détails à cause de la difficulté de trouver des patches similaires.
La nature de l'image à débruiter influe également sur l'efficacité de ce paramètre, qui est donc à fixer en fonction de l'image.
Notons l'article de \cite{Manjon:JMRI:2010} introduisant des patches adaptatifs de manière à débruiter une IRM présentant un bruit variable.
L'article de \cite{Kervrann:IJCV:2008} peut également être mentionné pour l'utilisation de cette technique.
Enfin, nous pouvons citer deux techniques utilisant la méthode SURE (\emph{Stein's unbiased risk estimate}) \cite{Stein:AS:1981} à partir de plusieurs exécutions des moyennes non-locales pour estimer l'image débruitée.
La première est celle de \cite{VanDeVille:TIP:2011} qui exécute les moyennes non-locales avec plusieurs taille de patches et de zone de recherche, et effectue une combinaison de ces résultats afin d'obtenir l'estimation la plus robuste possible en chaque élément de l'image.
Par ailleurs, \cite{Deledalle:JMIV:2011} utilise des patches de formes diverses et agrège les différentes estimations localement. 
De plus, ils utilisent une implémentation fondée sur la transformée de Fourrier rapide de manière à accélérer les traitements.
Ses résultats montrent une nette diminution du halo de bruit que l'on peut observer avec l'algorithme des moyennes non-locales original.
% Une technique de patches adaptatifs a été implémentée par \cite{Kervrann:TIP:2006} dans l'

% D'autres travaux ont cherché à améliorer ce filtre. 
% Certains ont accéléré les traitements en partant du principe qu'un voxel appartient à plusieurs patches différents \cite{Coupe:TMI:2008}, la valeur finale du voxel étant récupérée par un moyenne sur l'ensemble des patches.

\subsection{Usage des moyennes non-locales}

Depuis son apparition en 2005, les moyennes non-locales ont été utilisées dans d'autres domaines que le débruitage d'images.
Le domaine des problèmes inverses, par la nécessité d'une régularisation dans le cas de problèmes mal posés, a bénéficié de l'apport des moyennes non-locales.
Nous pouvons citer les travaux de \cite{Bougleux:ECCV:2008} sur cette thématique, ainsi que ceux de \cite{Mignotte:PRL:2008} en déconvolution et ceux de \cite{Rousseau:MIA:2010} en reconstruction de volumes 3D.
En déconvolution, les moyennes non-locales sont utilisées de manière à favoriser les images présentant un haut niveau de redondance, soit dans l'idéal des images invariantes au filtre non-local.

Enfin, les moyennes non-locales ont également été appliqué à la segmentation d'images.
Les travaux de \cite{Gilboa:MMS:2008} se sont focalisés sur la définition d'un opérateur gradient et et d'un opérateur divergence non-locaux de manière à étendre les techniques fondées sur des équations différentielles partielles (EDP) ou des approches variationnelles.
Les travaux de \cite{Bresson:2008} ont défini plusieurs fonctionnelles non-locales permettant une segmentation non-supervisée par contours actifs avec une approche variationnelle.
Les moyennes non-locales ont également été utilisées en propagation de labels à partir d'une base d'images dont la vérité terrain est connue. 
L'article de \cite{Rousseau:TMI:2011} décrit une méthodologie cherchant les patches similaires dans une base d'image, après une recalage affine et une égalisation d'histogramme.
Cette méthododologie a l'avantage de ne pas nécessiter de recalage non-linéaire, et base l'étape de fusion des labels sur les poids non-locaux calculés à travers la base.
Les moyennes non-locales ont également été introduites dans les lignes de niveaux par~\cite{Jung:SSVM:2011} et ont permis la segmentation de structures dont l'intensité varie de façon lisse, ce qui peut se révélé intéréssant dans le cas ou un biais est présent.
% Enfin, l'article de \cite{Wang:CMIG:2008} définit une première approche permettant de prendre en compte l'information non-locale dans FCM. 
% Elle est décrite à la section suivante.


% \subsection{Segmentation non-locale}

Une première utilisation des moyennes non-locales dans l'algorithme FCM a été définie par~\cite{Wang:CMIG:2008} avec une application à la segmentation cérébrale.
Cette méthodologie a été développée de manière indépendante de la notre.
Les auteurs utilisent une pondération entre information locale et information non-locale en redéfinissant la distance entre l'intensité d'un voxel et le centroïde d'une classe de la façon suivante :
\begin{equation}
D^{2}(\mathbf{y}_{j}, \mathbf{v}_{k}) = (1-\lambda_{j}) d^{2}_{l}(\mathbf{y}_{j}, \mathbf{v}_{k}) + \lambda_{j} d^{2}_{nl}(\mathbf{y}_{j}, \mathbf{v}_{k}) \label{nlfcm:wang:distance},
\end{equation}
où $D(\mathbf{y}_{j}, \mathbf{v}_{k})$ représente la distance entre l'intensité $\mathbf{y}_{j}$ du voxel $j$ et le centroïde $\mathbf{v}_k$ de la classe $k$.
Cette distance est une combinaison entre une distance locale $d_{l}(\mathbf{y}_{j}, \mathbf{v}_{k})$ et une distance non-locale $d_{nl}(\mathbf{y}_{j}, \mathbf{v}_{k})$.
Le paramètre $\lambda_{j}$ contrôle localement la proportion entre ces deux distances pour le calcul de la distance $D$.

La distance locale est calculée de la façon suivante :
\begin{equation}
d^{2}_{l}(\mathbf{y}_{j}, \mathbf{v}_{k}) = \frac{\sum_{l \in N{j}} \omega_{l} (\mathbf{y}_{j}, \mathbf{y}_{l}) \lVert \mathbf{y}_{l} - \mathbf{v}_{k} \rVert^{2}_{2}}{\sum_{l \in N{j}} \omega_{l} (\mathbf{y}_{j}, \mathbf{y}_{l})} \label{nlfcm:wang:distance:locale},
\end{equation}
où $N_{j}$ est un voisinage centré autour du voxel $j$, $\omega_{l} (\mathbf{y}_{j}, \mathbf{y}_{l}) = e^{-\frac{\lvert \mathbf{y}_{j} - \mathbf{y}_{l} \rvert}{\sigma^{2}}}$ est le poids local accordé au voxel $l$ et $\sigma^{2}$ est la variance sur $N_{j}$.
Cette distance locale représente donc une moyenne pondérée sur le voisinage $N_j$, les pondérations étant calculées à partir de la similarité entre le voxel central et ses voisins (similarité voxel à voxel et non similarité des patches).

La distance non-locale est calculée de la façon suivante : 
\begin{equation}
d^{2}_{nl}(\mathbf{y}_{j}, \mathbf{v}_{k}) = \sum_{j' = 1}^{N}\omega_{nl}(j,j') \lVert \mathbf{y}_{j'} - \mathbf{v}_{k} \rVert^{2}_{2},
\end{equation}
où $\omega_{nl}(j,j')$ représente le poids non-local entre les voxels $j$ et $j'$.

% En pratique, ce calcul de la distance non-locale est limité à une zone de recherche $\Omega^{R}_{j}$ centrée autour du voxel $j$.
Le paramètre $\lambda_{j}$ est calculé automatiquement en tenant compte de la similarité globale du patch $P^{I}_{j}$ au sein de $\Omega^{R}_{j}$. 
Plus il y a de voxels similaires au voxel central au sein de la zone de recherche $\Omega^R_j$, plus le poids de la distance non-locale dans le calcul de la distance générale sera important.
L'idée sous-jacente est qu'il existe des parties de l'image présentant peu de similaritée entre les voxels (par exemple, dans une IRM cérébrale, autour des sillons cérébraux) où il est plus efficace de prendre en compte une similarité entre les voxels directement plutôt qu'entre les patches.

% Nous montrerons que la correction du bruit présent dans une image ne nécessite pas de définir une telle distance, mais qu'il suffit d'incorporer les moyennes non-locales au sein du terme de régularisation de FCM pour obtenir une correction efficace.

\section{FCM non-local}
\label{sec:nlfcm}

Nous présentons dans cette section une extension de l'algorithme FCM incluant les moyennes non-locales au sein du terme d'attache aux données de manière à améliorer la prise en compte du biais en intensité et au sein du terme de régularisation de manière à améliorer celle du bruit.
% Dans la suite du manuscrit, l'implémentation des moyennes non-locales utilise un noyau de type gaussien $K(x) = e^{\frac{-x}{h}}$ et la norme euclidienne pour le calcul des similarités. 
% L'estimation de la fenêtre $h$ est inspiré de \cite{Coupe:TMI:2008}, laissant un paramètre $\alpha$ à évaluer.

\subsection{Terme d'attache aux données}
\label{subsec:nl:dd}

Comme précisé dans la section \ref{subsec:fcm:def}, l'algorithme FCM considère que les centroïdes des classes sont invariants au sein de l'espace de l'image, le rendant sensible à l'inhomogénéité des intensités.
Nous avons vu à la section~\ref{subsec:fcm:bias} qu'il est alors nécessaire de définir un modèle du biais explicite de manière à en prévenir les effets sur la segmentation.
Cependant, de nombreuses hypothèses doivent être faites pour pouvoir estimer ce biais, notamment sur sa nature multiplicative, la nature lente des variations d'intensité (valable que pour les inhomogénéités dues à l'imageur), la modélisation du biais lui même (polynomiales, B-Splines,\ldots{}) ou encore la définition d'un champ de biais unique ou d'un champ par tissu.

\begin{figure}[!t]
\begin{center}
\subfigure[]{\includegraphics[height=42mm]{eps/chapitre3/BigBox.eps}\label{fig:nlm:seglocale:a}}
\subfigure[]{\includegraphics[height=42mm]{eps/chapitre3/LittleBox.eps}\label{fig:nlm:seglocale:b}}
\end{center}
\caption[Modélisation locale pour la segmentation]{\emph{Modélisation locale de l'image. (a) Deux sous-volumes distincts présentent deux histogrammes différents conduisant à classer une même intensité (marquée par la barre verticale) dans deux classes différentes. (b) Illustration de la mauvaise estimation issue de la définition de sous-volumes trop petits. Images issues de~\cite{Scherrer:2008}}}
\label{fig:nlm:seglocale}
\end{figure}

Une approche locale de la segmentation est une alternative à explorer car elle permet une estimation des modèles d'intensité dans des sous-volumes du volume complet sans avoir recours à une estimation explicite du biais, évitant de poser les nombreuses hypothèses citées précédemment.
Une même intensité peut donc être étiquettée différemment selon l'estimation locale du modèle (voir en Figure~\ref{fig:nlm:seglocale:a}).
La taille des sous-volumes est cependant cruciale car un sous-volume trop important sera sensible à l'inhomogénéité tandis qu'un volume trop réduit risque de conduire à une mauvaise évaluation du modèle d'intensité local (voir en Figure~\ref{fig:nlm:seglocale:b}). 
Dans le cas de l'IRM cérébrale, une fenêtre trop petite peut, par exemple, conduire à une mauvaise évaluation dans des zones de grande concentration de matière blanche.
En effet, trop peu de voxels représentant la matière grise ou le LCR (voir aucun) seraient alors pris en compte pour permettre une estimation fiable des centroïdes des classes représentant ces tissus. 
Or l'algorithme FCM reposant sur l'estimation de ces centroïdes, cela conduirait à une classification finale faussée.

Définie dans le cadre de FCM, cette notion de modèle local se traduit par la minimisation de la fonction d'énergie suivante : 
\begin{equation}
J = \sum_{j \in \Omega} \sum_{k=1}^{C} u^{q}_{jk} \lVert \mathbf{y}_{j} - \mathbf{v}_{jk} \rVert_{2}^{2}. \label{eq:fcm:local}
\end{equation}
La différence entre cette fonction coût et celle de l'algorithme FCM classique réside dans l'introduction de centroïdes locaux $\mathbf{v}_{jk}$ fournissant une évaluation du modèle d'intensité au voxel $j$.
Des approches par recouvrement de sous-volumes ont été définies comme celle de \cite{Zhu:NeuroImage:2003} comportant une étape de classification par FCM proprement dite, puis une étape de fusion de l'information.
De même, \cite{Scherrer:TMI:2009} introduit des modèles markoviens locaux par la division de l'image en sous-volumes et estime ces modèles en coopération avec les modèles voisins afin d'assurer la cohérence de l'ensemble de la segmentation.
Cependant, ces deux approches peuvent être considérées comme un déplacement du problème de l'évaluation d'un modèle global vers celui de la fusion de plusieurs sous-modèles.

A notre connaissance, aucune méthode ne permet la prise en compte des modèles voisins sans cette étape de fusion.
Cependant, l'approche non-locale permet de mettre en \oe uvre une pondération entre les différents voxels de l'image en fonction de la similarité de leur patch.
En faisant l'hypothèse que deux voxels dont les patches sont similaires font partie du même tissu, il est possible d'utiliser ces pondérations pour prendre en compte les modèles voisins (c'est-à-dire l'estimation des centroïdes locaux aux voxels voisins) pour diminuer le risque qu'entraîne une mauvaise évaluation des centroïdes locaux.
Un terme d'attache aux données non-local est alors défini de la façon suivante :
\begin{equation}
J_{NL-FCM} = \sum_{j \in \Omega} \sum_{k=1}^{C} u_{jk}^{q} \sum_{l \in \Omega^{R_d}_{j}} \omega_{nl}(j, l) \Vert \mathbf{y}_{j} - \mathbf{v}_{lk} \Vert_2^{2} \label{eq:nlfcm:dd}.
\end{equation}
Le terme $\omega_{nl}(j, l)$ est calculé selon l'équation \ref{nlfcm:poids:definition} et $h$ selon la méthode définie par \cite{Coupe:TMI:2008}.
% Ce terme d'attache aux données permet une meilleure robustesse de la segmentation en cas de mauvaise évaluation de certains modèles locaux.

Deux différences sont notables par rapport à la définition standard de FCM (donnée par l'équation \ref{eq:fcm}).
La première a déjà été évoquée et consiste à remplacer le terme global des centroïdes $\mathbf{v}_{k}$ dans FCM par un terme local $\mathbf{v}_{jk}$ de manière à bénéficier d'une évaluation locale du modèle d'intensité.
Ces centroïdes locaux sont calculés à partir d'un sous-volume $M_j$ centré autour du voxel $j$ et dont l'influence de la taille est discutée plus loin.
La deuxième différence est que chaque modèle inclus dans la zone de recherche $\Omega^{R_d}_{j}$ va avoir une influence sur la classification finale du voxel $j$.
Un modèle étant défini pour chaque voxel de l'image, cette proportion est donc contrôlée par le poids non-local $\omega_{nl}(j,j')$ traduisant la similarité entre le voxel $j$ et chacun des voxels $j'$ inclus dans la zone de recherche.  

\subsection{Terme de régularisation}
\label{subsec:nl:reg}

Comme vu dans la section~\ref{subsec:fcm:noise}, le terme de régularisation de la fonction d'énergie de FCM s'apparente la plupart du temps à l'expression d'un filtrage médian ou par moyenne pour lisser la segmentation, comme dans l'article de~\cite{Ahmed:TMI:2002}.
L'article de~\cite{Pham:CVIU:2001} a une approche légèrement différente étant donné que le lissage ne se fait qu'en prenant en compte l'ensemble des classes exceptée la classe courante, ce qui permet de favoriser le terme d'attache aux données si la classe courante est bien représentée dans le voisinage. 
Plusieurs articles ont introduit différentes stratégies permettant de prendre en compte la similarité entre les voxels (introduisant ainsi une pondération entre les voxels d'un même voisinage), mais cette prise en compte nécessite le calcul de plusieurs variables additionnelles ou la définition d'une image intermédiaire (voir la sous-section \ref{subsec:fcm:noise}).

Les moyennes non-locales sont particulièrement intéressantes dans cette situation, car elles fournissent les outils nécessaires à une pondération relativement aisée des voxels d'intêret au sein de la zone de recherche.
Le rôle de poids non-locaux est donc de sélectionner les voxels les plus pertinents au sein de la zone de recherche pour effectuer la régularisation en fonction de leur degré de similarité avec le voxel courant.
L'hypothèse que nous faisons est que si les patches de deux voxels sont similaires, alors ils appartiennent au même tissu.
L'objectif est d'obtenir une régularisation plus lisse de façon adaptative.

Le terme de régularisation défini dans cette section s'inspire de celui de \cite{Pham:CVIU:2001} (dont la définition est donnée par l'équation \ref{eq:rfcm}), calculant la proportion d'une classe au sein d'un voxel en prenant en compte la proportion des autres classes dans le voisinage.
Le terme de régularisation non-local est exprimé de la façon suivante :
\begin{equation}
J_{NL\textrm{-}Reg} = 
\frac{\beta}{2} \sum_{j \in \Omega} \sum_{k=1}^{C} u_{jk}^{q} \sum_{j' \in \Omega^{R_{r}}_{j}} \omega_{nl}(j, j') \sum_{l \in L_{k}} u_{j'l}^{q} \label{eq:nlfcm:reg}.
\end{equation}

Rappelons que $L_{k}=[\![1,C]\!]\setminus \{k\} = \{1, \ldots, k-1, k+1, \ldots, C\}$ représente l'ensemble des classes exceptée la classe dont on effectue la régularisation et $\Omega^{R_{r}}_{j}$ représente la zone de recherche centrée autour du voxel $j$ destinée à calculer les poids non-locaux pour la régularisation.
Le paramètre $\beta$ contrôle les poids respectifs entre le terme de régularisation et le terme d'attache aux données au sein de la fonction d'énergie.
Le terme $\omega_{nl}(j, l)$ est calculé selon l'équation \ref{nlfcm:poids:definition} et $h$ selon la méthode définie par \cite{Coupe:TMI:2008}.

\subsection{Algorithme non-local complet}

L'association des deux termes non locaux donne un algorithme de segmentation non local complet permettant de prendre en compte l'inhomogénéité en intensité et le bruit de l'image. 
La fonction d'énergie devient donc : 
\begin{equation}
\begin{array}{l l}
J_{NL\textrm{-}R\textrm{-}FCM} & = J_{NL\textrm{-}FCM} + J_{NL\textrm{-}Reg} \\
 & = \left\{ 
\begin{array}{c}
\sum_{j \in \Omega} \sum_{k=1}^{C} u_{jk}^{q} \sum_{j' \in \Omega^{R_d}_{j}} \omega_{nl}(j, j') \Vert \mathbf{y}_{j} - \mathbf{v}_{j'k} \Vert_2^{2} + \\
\frac{\beta}{2} \sum_{j \in \Omega} \sum_{k=1}^{C} u_{jk}^{q} \sum_{j'' \in \Omega^{R_{r}}_{j}} \omega_{nl}(j, j'') \sum_{l \in L_{k}} u_{j'l}^{q} \label{eq:nlfcm:reg}.
\end{array}
\right.
\end{array}
\end{equation}
Il est important de noter que les poids $\omega_{nl}(\cdot,\cdot)$ présents dans le terme d'attache aux données et le terme de régularisation sont distincts car les zones de recherche $\Omega^{R_d}_{j}$ et $\Omega^{R_r}_{j}$ ne sont pas nécessairement identiques.
Etant donné que le biais est un artéfact lisse variant lentement le long de l'image, la zone de recherche $\Omega^{R_d}_{j}$ peut être choisi aussi grande que possible.
Par contre, la correction du bruit nous impose un plus petit rayon pour $\Omega^{R_r}_{j}$ afin de limiter la prise en compte de patches peu similaires lors du calcul des poids.

Les différentes étapes permettant de minimiser la fonction d'énergie du FCM non-local sont les suivantes : 
\begin{enumerate}
\item Calculer les poids non-locaux $\omega_{nl}$ pour le terme d'attache aux données et le terme de régularisation.
\item Calculer les centroïdes $\mathbf{v}_{jk}$ pour tout $(j,k) \in [\![ 1,C ]\!]\times \Omega$ selon : 
\begin{equation}
\mathbf{v}_{jk} = \frac{\sum_{l \in M_{j}} u_{lk}^{q} \mathbf{y}_{l}}{\sum_{l \in M_{j}} u_{lk}^{q}}.
\end{equation}
\item Calculer $u_{jk}$ pour tout $(j,k) \in [\![ 1,C ]\!]\times \Omega$ selon : 
\begin{equation}
 u_{jk} = \frac{(\sum_{j' \in \Omega^{R_d}} \omega_{nl}(j, j')\lVert \mathbf{y}_{j} - \mathbf{v}_{k} \rVert^{2} + \beta \sum_{j'' \in \Omega^{R_r}} \omega_{nl}(j, j'') \sum_{m \in M_{k}} u_{j''m}^{q})^{\frac{-1}{q-1}}}{\sum_{k=1}^{C}(\sum_{j' \in \Omega^{R_d}} \omega_{nl}(j, j')\lVert \mathbf{y}_{j} - \mathbf{v}_{k} \rVert^{2} + \beta \sum_{j'' \in \Omega^{R_r}} \omega_{nl}(j, j'') \sum_{m \in M_{k}} u_{j''m}^{q})^{\frac{-1}{q-1}}}
\end{equation}
\item Répéter jusqu'à un minimum local de la fonction d'énergie : 
\begin{itemize}
\item Recalculer $\mathbf{v}_{jk}$ pour tout $(j,k) \in [\![ 1,C ]\!]\times \Omega$.
\item Recalculer $u_{jk}$ pour tout $(j,k) \in [\![ 1,C ]\!]\times \Omega$.
\end{itemize}

\end{enumerate}

La donnée d'entrée de l'algorithme est l'image à segmenter (fournissant $\Omega$ et les valeurs $\mathbf{y}$).
Les paramètres déterminant son comportement sont : $C$ (le nombre de classes), $\beta$ (qui contrôle le rapport entre le terme d'attache aux données et le terme de régularisation), la taille de la zone de recherche pour le calcul des poids non-locaux destinés à la régularisation $\Omega^{R_r}_j$, la taille de la zone de recherche pour le calcul des poids non-locaux destinés au terme d'attache aux données $\Omega^{R_d}_j$, la taille des sous-volumes $M_j$ permettant l'estimation des modèles locaux et le paramètre de lissage $\alpha$ intervenant dans l'estimation de $h$.
\begin{table}[!t]
\begin{center}

\begin{tabular}{|c|c|}

\hline
NL-FCM & Terme d'attache aux données non-local \\
\hline
NL-Reg & Terme de régularisation non-locale \\
\hline
NL-R-FCM & FCM avec attache aux données et régularisation non-locals \\
\hline

\end{tabular}
\caption[Récapitulatif des acronymes utilisés pour FCM non-local]{\emph{Récapitulatif des acronymes utilisés pour les différentes versions du FCM non-local.}\label{fig:nlfcm:acronymes}}
\end{center}
\end{table}
Le tableau~\ref{fig:nlfcm:acronymes} résume l'ensemble des terminologies utilisées pour désigner les différentes version de l'algorithme.


\section{Validations}
\label{sec:nlfcm:validations}

Les expériences sont menées tout d'abord sur des images simulées fournies par la base BrainWeb \cite{Kwann:VBC:1996, Cocosco:NeuroImage:1997}, puis sur des cas réels fournis par la base IBSR (\emph{Internet Brain Segmentation Repository}).

Considérant tout d'abord la base BrainWeb, trois séries d'expériences sont menées de manière à déterminer le comportement de l'algorithme FCM non-local :
\begin{enumerate}
        \item Évaluation du terme d'attache aux données non-local en utilisant des images ayant 20~\% d'inhomogénéité en intensité (voir la section~\ref{sec:brainweb:nlData}).
        \item \'Evaluation du terme de régularisation non-locale en utilisant des images ayant différents niveaux de bruit ricien (voir la section~\ref{sec:brainweb:nlReg}).
        \item Évaluation de l'algorithme non-local complet en utilisant des images ayant à la fois 20~\% d'inhomogénéité en intensité et différents niveaux de bruit (voir la section~\ref{sec:brainweb:nlAll}).
\end{enumerate}
Par la suite, l'algorithme FCM non-local complet est appliqué à l'ensemble de la base IBSR.

L'évaluation des performances des différents algorithmes est faite par comparaison de la segmentation estimée avec la vérité terrain fournie par les bases d'images respectives.
La qualité de la segmentation est mesurée par le calcul de l'indice de similarité Dice : 
$$
Dice = \frac{2\cdotp VP}{2\cdotp VP + FP + FN},
$$
où $VP$ est le nombre de vrais positifs, $FP$ le nombre de faux positifs et $FN$ le nombre de faux négatifs.
Cet indice calcule un taux de recouvrement entre la vérité terrain et la segmentation automatique.

De plus, l'évaluation inclut à chaque étape une comparaison avec d'autres méthodes de classification reposant sur les champs et chaînes de Markov.
Ces méthodes sont : 
\begin{itemize}
        \item SPM5 de Ashburner \emph{et al.}~\cite{Ashburner:NeuroImage:2005},
        \item EMS de Van Leemput \emph{et al.}~\cite{VanLeemput2:TMI:1999},
        \item HMC de Bricq \emph{et al.}~\cite{Bricq:MIA:2008}.
\end{itemize}
Ces trois méthodes incluent une correction du biais et un terme de régularisation markovien.


\subsection{BrainWeb}
\label{subsec:brainweb}

Le site de BrainWeb~\footnote{\url{http://www.bic.mni.mcgill.ca/brainweb/}} permet de simuler des IRM cérébrales avec différents niveaux de bruit et d'inhomogénéité et fournit la vérité terrain correspondante, permettant de comparer différents algorithmes de segmentation.
% La connaissance de la vérité terrain permet de comparer différents algorithmes de segmentation.
Les images fournies par la base sont issues de 27 acquisitions réalisées sur un même individu.
Elles sont ensuite moyennées pour construire le volume initial auquel sont ensuite appliqués divers traitements pour créer le fantôme~\cite{Collins:TMI:1998}.
Les images utilisées sont de taille $181\times181\times217$ avec des voxels isotropes de $1$ mm de côté.
Le bruit est de type ricien d'un niveau allant de $0$ à $9$~\% (c'est à dire une amplitude du bruit allant jusqu'à $9$~\% de la valeur maximale de l'image).

\subsubsection{Évaluation du terme d'attache aux données}
\label{sec:brainweb:nlData}

L'algorithme est initialisé grâce à l'atlas 452 T1 fourni par l'\emph{International Consortium for Brain Mapping} (ICBM).
Il est recalé sur l'image BrainWeb de manière linéaire et affine.
Il permet d'initialiser de manière fiable les moyennes locales avant le lancement de NL-FCM proprement dit et fournit une première estimation de la classification des tissus.

\paragraph*{\'Etude des paramètres}

Les paramètres étudiés sont la taille de la zone de recherche $\lvert \Omega^{R_{d}}_{j} \rvert$ pour le calcul des poids non-locaux et la taille du voisinage $M_j$ pour le calcul des centroïdes locaux.
On pose $n_j$ et $m_j$ tels que $\lvert \Omega^{R_{d}}_{j} \rvert = (2 \cdot n_j + 1)^{3}$ et $\lvert M_{j} \rvert = (2 \cdot m_j + 1)^{3}$.

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/Brainweb_Bias_Couple_GM.eps}}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/Brainweb_Bias_Couple_WM.eps}}
        \end{center}

        \caption[Influence des paramètres $M_j$ et $\Omega^{R_d}_j$ du terme d'attache aux données non-locale]{\emph{Influence des paramètres $M_j$ et $\Omega^{R_d}_j$ du terme d'attache aux données non-locale. L'accroissement de la zone de recherche $\Omega^{R_d}_{j}$ permet de compenser les éventuelles mauvaises évaluation du modèle dues à des sous-volumes $M_j$ trop réduits. (a) Coefficient Dice pour la matière grise. (b) Coefficient Dice pour la matière blanche. $n_j = 4$ : $+$, $n_j = 5$ : $\diamond$, $n_j = 6$ : $\circ$, $n_j = 7$ : $\times$, $n_j = 8$ : $\bigtriangledown$, $n_j = 9$ : $\star$.}}

        \label{FIG:PARAM:BRAINWEB:BIAS}

\end{figure}

La figure~\ref{FIG:PARAM:BRAINWEB:BIAS} présente les résultats avec différents sous-volumes et différentes zones de recherche.
La première partie de la courbe, de $m_j = 5$ à $m_j = 9$, montre comme attendu que pour une même valeur de $m_j$, l'accroissement de la taille de zone de recherche permet d'obtenir une meilleure segmentation par la prise en compte des modèles voisins et la pondération en résultant.
Cependant, un fléchissement est observé à $m_j = 10$ pour $n_j = 8$ et $n_j = 9$.
Les différents travaux sur les moyennes non-locales pointaient la faible efficacité d'une zone de recherche trop grande \cite{Salmon:2010,Kervrann:TIP:2006}, surtout dans le cas d'un noyaux à support infini tel que le noyau gaussien. 
En effet, les voxels de faible similarité avec le voxel courant sont pris en compte dans le calcul des pondérations, ce qui entraîne un biais dans le calcul de l'attache aux données locale.
Ceci, couplé à un voisinage $M_j$ trop important rendu le calcul des centroïdes locaux sensibles à l'inhomogénéité en intensité, explique la baisse des performances de l'algorithme FCM non-local avec des paramètres $n_j$ et $m_j$ trop importants.

Les paramètres finaux choisis pour la suite des expérience sont un compromis entre la performance et le temps de calcul de l'algorithme.
En effet, les deux voisinages $M_j$ et $N_j$ étant de taille importantes, ils entraînent des temps de calculs de l'ordre de plusieurs heures.
Les paramètres $n_j$ et $m_j$ donc fixé à : $(n_j, m_j) = (8, 8)$ (voir la figure~\ref{FIG:PARAM:BRAINWEB:BIAS}) car le résultat de ce couple de valeur est proche du résultat optimale et permet la diminution du temps de calcul.

\paragraph*{Comparaison à d'autres méthodologies}

Le terme d'attache aux données non-locale a été comparé à la version classique de l'algorithme FCM ainsi qu'à SPM5, EMS et HMC. 
La table~\ref{TAB:DICE:BRAINWEB:BIAS} récapitule les taux de recouvrement obtenus avec les différentes méthodes de segmentation.
La figure~\ref{FIG:VIEW:BRAINWEB:BIAS} illustre les performances du terme d'attache aux données non-locale dans un environnement présentant uniquement un biais en intensité.

\begin{table}[!t]
\begin{center}
\begin{tabular}{|l | *{2}{c|}}
	\hline
	Méthodes &  Matière grise & Matière blanche \\
	\hline
	SPM5 \cite{Ashburner:NeuroImage:2005}& 91.4 & 91.3 \\
	EMS \cite{VanLeemput2:TMI:1999}& 83.7 & 86.9\\
	HMC \cite{Bricq:MIA:2008} & 94.0 & 95.9\\
	FCM & 69.2 & 75.83\\
	NL-FCM & \fbox{95.68} & \fbox{96.35}\\
	\hline 
\end{tabular}
% \vspace{2mm}
\caption[Coefficients Dice issus de l'application de différentes segmentations à une image pondérée en T1 présentant un biais en intensité de 20~\%]{\emph{Application de différentes segmentations à une image pondérée en T1 présentant un biais en intensité de 20~\%. Comparaison des différents coefficients Dice pour la segmentation de la matière grise et de la matière blanche. Paramètres pour NL-FCM : $n_j = 8$ et $m_j = 8$. \label{TAB:DICE:BRAINWEB:BIAS}}}
\end{center}
\end{table}

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_Bias_T1.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_Bias_truth.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_Bias_classicfcm.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_Bias_nlfcm.eps}}
        \end{center}

        \caption[Résultats de la segmentation d'une image T1 présentant un biais en intensité]{\emph{Résultats de la segmentation d'une image T1 présentant un biais en intensité. (a) Coupe d'une image T1. (b) Vérité terrain. (c) Segmentation par FCM classique. (d) Segmentation par FCM non-local.}}

        \label{FIG:VIEW:BRAINWEB:BIAS}

\end{figure}

Nous pouvons observer que l'algorithme FCM non local obtient les meilleurs scores selon les taux de recouvrement.
Elle apporte une prise en compte du biais en intensité à l'algorithme FCM sans en nécessiter une évaluation explicite.


\subsubsection{Evaluation du terme de régularisation}
\label{sec:brainweb:nlReg}

L'algorithme NL-Reg est initialisé par un algorithme des K-Moyennes de manière à obtenir une première estimation de la distribution en intensité et les cartes de probabilité sont initialisées à $\frac{1}{C}$ ($C$ étant le nombre de classes recherchées).
Dans un premier temps, des tests sont réalisés sur une image bruitée à $5$~\% afin d'évaluer l'influence des paramètres non-locaux sur la segmentation.
Enfin, différentes segmentations sont réalisées avec les paramètres optimaux obtenus pour étudier le comportement du terme de régularisation non-local en fonction du bruit.

\paragraph*{Influence des paramètres non-locaux}

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/nlreg_Dice_Smooth.eps}}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/nlreg_Dice_HWVS.eps}\label{FIG:PARAM:BRAINWEB:NOISE:HWVS}}
        \end{center}
        
        \caption[Etude de l'influence du paramètre de lissage $\alpha$ et du rayon de la zone de recherche $\Omega^{R_{r}}_{j}$]{\emph{(a) Coefficient Dice en fonction du paramètre de lissage $\alpha$. (b) Coefficient Dice en fonction du rayon de la zone de recherche $\Omega^{R_{r}}_{j}$. LCR : $\bigtriangledown$, matière grise : $\times$, matière blanche : $\circ$.}}
        
        \label{FIG:PARAM:BRAINWEB:NOISE}

\end{figure}

Les paramètres testés sont la taille de la zone de recherche $\lvert \Omega^{R_{r}}_{j} \rvert$ ainsi que le paramètre de lissage $\alpha$.
La figure \ref{FIG:PARAM:BRAINWEB:NOISE} fournit une indication de l'influence de ces deux paramètres sur la segmentation.
Le paramètre de lissage $\alpha$ a une forte influence sur la segmentation s'il est situé entre $0$ et $1.5$, puis un phénomène de convergence est observé. 
Dans la suite du manuscrit, il sera fixé à $1.5$.
Concernant la taille de la zone de recherche, la figure~\ref{FIG:PARAM:BRAINWEB:NOISE:HWVS} montre un maximum du taux de recouvrement avec un rayon de $2$ voxels pour la matière blanche et la matière grise, et un maximum avec un rayon de $3$ voxels pour le LCR.
Le rayon de la zone de recherche $\Omega^{R_r}_{j}$ est donc fixée à $2$ pour la suite du manuscrit. 

\paragraph*{Comparaison par rapport à d'autres méthodologies}

La comparaison inclut une évaluation des méthodes FCM classique et RFCM \cite{Pham:CVIU:2001} en plus des méthodes markoviennes.
Elle est effectuée en simulant des segmentations avec un bruit ricien allant de $0$ à $9$~\%. 

La figure~\ref{FIG:VIEW:BRAINWEB:NOISE} présente une coupe axiale d'une image T1 et les segmentations correspondantes pour les algorithmes FCM classique, RFCM \cite{Pham:CVIU:2001} et NL-Reg.
L'apport de RFCM par rapport à l'algorithme FCM classique est visible par l'absence d'artéfacts de segmentation dus au bruit (par exemple : voxels classés comme matière grise au milieu de la matière blanche).
Cependant, l'effet de lissage apporté par cette régularisation a pour effet de gommer les aspérités dans certaines parties de l'image. 
Par exemple, une comparaison visuelle avec la vérité terrain montre une sous-segmentation du LCR au sein des sillons.
Le terme de régularisation non-local permet de remédier à cela grâce à la pondération introduite par les poids non-locaux, ce qui est illustré par les figures~\ref{NLREG:ZOOM:TRUTH}, \ref{NLREG:ZOOM:RFCM} et \ref{NLREG:ZOOM:NLREG} montrant un zoom sur une zone particulière du cerveau.

De plus, le  tableau~\ref{TAB:DICE:BRAINWEB:NOISE} montre une meilleure performance des algorithmes FCM que des algorithmes basés sur les champs et chaînes de Markov. 
La figure~\ref{FIG:DICE:BRAINWEB:NOISE}, comparant les coefficients Dice obtenus par les algorithmes en fonction du niveau de bruit, confirme cette observation.
Elle montre que le terme de régularisation non-local fournit une segmentation plus consistante à partir d'un niveau de bruit de $5$~\%.

\begin{table}[!t]
\begin{center}
\begin{tabular}{|l | *{3}{c|}}
	\hline
	Méthodes & LCR & Matière grise & Matière blanche \\
	\hline
	FCM & 90.46 & 84.36 & 85.48\\
	RFCM & 92.09 & 91.12 & 92.91\\
% 	R-FCM with adaptive weights & 92.76 & 91.09 & 92.49\\
% 	NL-Reg without adaptive weights & 92.22 & 92.22 & 94.12\\
	NL-Reg & \fbox{93.63} & \fbox{93.35} & \fbox{94.77}\\
	SPM5 \cite{Ashburner:NeuroImage:2005}& 54.2 & 85.1 & 87 \\
	EMS \cite{VanLeemput2:TMI:1999}& 89.6 & 86.9 & 90.9\\
	HMC \cite{Bricq:MIA:2008} & 68 & 86.5 & 87.1\\
	\hline 
\end{tabular}
% \vspace{2mm}
\caption[Coefficients Dice issus de différentes segmentations d'une image pondérée en T1 avec un bruit ricien de $9$~\%]{\emph{Application de différentes segmentations à une image pondérée en T1 avec un bruit ricien de $9$~\%. Comparaison des différents coefficient Dice pour le LCR, la matière grise et la matière blanche.\label{TAB:DICE:BRAINWEB:NOISE}}}
\end{center}
\end{table}

\begin{figure}[!t]
\begin{center}
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_T1.eps}}  
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_truth.eps}}\\
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_classicfcm.eps}}  
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_rfcm.eps}}  
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_nlregfcm.eps}}\\
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_truth_zoom.eps}\label{NLREG:ZOOM:TRUTH}}  
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_rfcm_zoom.eps}\label{NLREG:ZOOM:RFCM}}  
\subfigure[]{\includegraphics[height=45mm, angle=180]{eps/chapitre3/Brainweb_Noise_nlregfcm_zoom.eps}\label{NLREG:ZOOM:NLREG}}\\
\caption[Aperçu de différentes segmentations d'une image pondérée en T1 avec un bruit ricien de $5$~\%]{\emph{(a) Coupe axiale d'une image pondérée en T1 avec un bruit ricien de $5$~\%. (b) Vérité terrain. (c) Segmentation par FCM classique. (d) Segmentation par RFCM~\cite{Pham:CVIU:2001}. (e) Segmentation par NL-Reg. (f) Zoom sur la vérité terrain. (g) Zoom sur la segmentation par RFCM. (h) Zoom sur la segmentation par NL-Reg.}\label{FIG:VIEW:BRAINWEB:NOISE}}
\end{center}
\end{figure}

\begin{figure}[!t]
\begin{center}
\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/GM_Noise_Dice_Brainweb.eps}}  
\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/WM_Noise_Dice_Brainweb.eps}}  
\caption[\'Evolution du coefficient Dice en fonction du bruit pour différentes méthodologies]{\emph{\'Evolution du coefficient Dice en fonction du bruit pour différentes méthodologies. (a) Matière grise. (b) Matière blanche. Légende : SPM5 : $\circ$, EMS : $\times$, HMC : $+$, FCM : $\diamond$, RFCM : $\bigtriangledown$, NL-Reg : $\star$.}\label{FIG:DICE:BRAINWEB:NOISE}}
\end{center}
\end{figure}

\subsubsection{Association des termes d'attache aux données et de régularisation non-locaux}
\label{sec:brainweb:nlAll}

% L'association des termes non-locaux devrait permettre de segmenter des images corrompues par un biais en intensité et par un bruit ricien. 
% Au cours de ces expériences, le biais en intensité est de l'ordre de 20\% et le bruit est de type Ricien allant de 1\% à 9\%.
Dans cette section, l'association des termes d'attache aux données et de régularisation non-locaux est évaluée.
Les images utilisées présentent un biais en intensité de $20$~\% et un bruit de type ricien d'un niveau allant de $0$~\% à $9$~\%.
Les paramètres considérés sont les suivants : 
\begin{itemize}
\item la taille des sous-volumes destinés à évaluer le modèle de la distribution d'intensité est fixée à $M_j = 17\times17\times17$, 
\item la taille de la zone de recherche destinée au calcul des poids non-locaux du terme d'attache aux données est fixée à $\Omega^{R_{d}}_{j} = 17\times17\times17$, 
\item la taille de la zone de recherche destinée au calcul des poids non-locaux du terme de régularisation est fixée à $\Omega^{R_{r}}_{j} = 5\times5\times5$,
\item le paramètre de lissage pour le calcul des poids non-locaux est fixé à $\alpha = 1.5$.
\end{itemize}

Les résultats sont fournis par la figure~\ref{FIG:VIEW:BRAINWEB:ALL} et la table~\ref{TAB:DICE:BRAINWEB:ALL}.
L'utilisation du terme d'attache aux données non-local permet d'améliorer largement les performances par comparaison avec l'algorithme FCM original.
De plus, l'ajout du terme de régularisation non-local permet de combiner les avantages des deux termes et de fournir une segmentation consistante malgré la présence d'un biais en intensité et de bruit.
% De plus, l'utilisation simultanée des termes non-locaux amène une amélioration conjointe des résultats et permet d'obtenir de meilleurs résultats finaux.

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_All_T1.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_All_Truth.eps}}\\
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_All_NLReg.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_All_NLFCM.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/Brainweb_All_NLRFCM.eps}}
        \end{center}

        \caption[Résultats de la segmentation d'une image T1 présentant une inhomogénéité en intensité et un bruit ricien de $9$~\%]{\emph{Résultats de la segmentation d'une image T1 présentant une inhomogénéité en intensité et un bruit ricien de $9$~\%. (a) Coupe d'une image T1. (b) Vérité terrain. (c) Segmentation par NL-Reg. (d) Segmentation par NL-FCM. (e) Segmentation par NL-R-FCM.}}

        \label{FIG:VIEW:BRAINWEB:ALL}

\end{figure}

\begin{table}[!t]
\begin{center}
\begin{tabular}{|l | *{2}{c|}}
	\hline
	Méthodes & Matière grise & Matière blanche \\
	\hline
	SPM5 \cite{Ashburner:NeuroImage:2005} & 85.1 & 87 \\ 
	EMS \cite{VanLeemput2:TMI:1999} & 86.9 & 87.1 \\
	HMC \cite{Bricq:MIA:2008} & \fbox{86.5} & \fbox{90.9}  \\
	\hline
	FCM \cite{Pham:TMI:1999} &  62.09 & 69.98\\
	NL-Reg  & 64.25 & 72.16\\
	NL-FCM & 82.0 & 84.7\\
	NL-R-FCM & \fbox{86.5} & 89.2\\
	\hline 
\end{tabular}
\vspace{2mm}
\caption[Coefficients Dice issus différentes segmentation sur une image T1 issue de BrainWeb avec un bruit Ricien à $9$~\% et un biais en intensité de $20$~\%]{Application de différentes segmentation sur une image T1 issue de BrainWeb avec un bruit Ricien à $9$~\% et un biais en intensité de $20$~\%. Comparaison des différents coefficient Dice pour la matière grise et la matière blanche (Coefficients Dice pour SPM5, EMS et HMC issus de \cite{Bricq:MIA:2008}).\label{TAB:DICE:BRAINWEB:ALL}}
\end{center}
\end{table}

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/Brainweb_All_GM.eps}}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/Brainweb_All_WM.eps}}\\
        \end{center}

        \caption[\'Evolution du coefficient Dice en fonction du bruit pour différentes méthodologies]{\emph{\'Evolution du coefficient Dice en fonction du bruit pour différentes méthodologies. (a) Matière grise. (b) Matière blanche. Légende : SPM5 : $\circ$, EMS : $\times$, HMC : $+$, NL-Reg : $\diamond$, NL-FCM : $\bigtriangledown$, NL-R-FCM : $\star$ (Coefficients Dice pour SPM5, EMS et HMC issus de \cite{Bricq:MIA:2008}).}}

        \label{FIG:DICE:BRAINWEB:ALL}

\end{figure}

La figure~\ref{FIG:DICE:BRAINWEB:ALL} montre l'évolution des performances des différentes méthodologies avec un bruit ricien allant de $0$ à $9$~\% dans le cas de la matière blanche et de la matière grise. 
Ces deux graphiques illustrent la similarité des performances de la méthode non-locale par rapport aux algorithmes basées sur les champs et chaînes de Markov.
Une différence significative n'est observable qu'à partir d'un niveau de bruit élevé ($7$~\%) et seulement dans le cas de la matière blanche. 
De plus, dans ce cas précis, la méthode HMC semble légèrement supérieure.

\subsection{IBSR}
\label{sec:ibsr}

Cette base, également disponible sur internet~\footnote{\url{http://www.cma.mgh.harvard.edu/ibsr/}}, est fournie par le Center for Morphometric Analysis du Massachussetts General Hospital.
Elle est composée de 18 volumes pondérés en T1 acquis sur des patients sains.
La taille de ces volumes est $128\times256\times256$ et chaque image dispose d'une vérité-terrain, comprenant le LCR, la matière blanche et la matière grise, réalisée par des experts.

\begin{table}[!t]
\begin{center}
\begin{tabular}{|l|c p{1.5cm}|c p{1.5cm}|}
	\hline
	Méthodes & \multicolumn{2}{|c|}{Matière Blanche (\%)} & \multicolumn{2}{|c|}{Matière Grise (\%)} \\
	& Moyenne & \centering{\'Ecart-Type} & Moyenne & \centering{\'Ecart-Type} \tabularnewline
	\hline
	SPM 5 \cite{Ashburner:NeuroImage:2005} & 85.27 & \centering 5.52 & 78.7 & \centering 13.98 \tabularnewline
	EMS \cite{VanLeemput2:TMI:1999} & 85.87 & \centering 2.27 & 78.94 & \centering 5.68 \tabularnewline
	HMC \cite{Bricq:MIA:2008} & \fbox{86.53} & \centering \fbox{1.73} & 79.94 & \centering 5.57 \tabularnewline
	\hline
	FCM \cite{Pham:TMI:1999} & 85.60 & \centering 3.81 & 83.21 & \centering 4.03 \tabularnewline
	R-FCM \cite{Pham:CVIU:2001} & 86.09 & \centering 2.75 & \fbox{84.08} & \centering 3.98 \tabularnewline
	\hline
	NL-Reg & 86.31 & \centering 3.18 & 83.18 & \centering 4.08 \tabularnewline
	NL-FCM & 84.68 & \centering 3.38 & 78.84 & \centering 4.07 \tabularnewline
	NL-R-FCM & 84.35 & \centering 3.38 & 83.22 & \centering \fbox{3.47} \tabularnewline
	\hline 
\end{tabular}
\vspace{2mm}
\caption[Moyennes des coefficients Dice (matière grise et matière blanche) obtenus pour différentes segmentations de la base d'image IBSR]{\emph{Moyennes des coefficients Dice (matière grise et matière blanche) obtenus pour différentes segmentations de la base d'image IBSR (Coefficients Dice pour SPM5, EMS et HMC issus de \cite{Bricq:MIA:2008}).\label{TAB:DICE:IBSR}}}
\end{center}
\end{table}

\begin{figure}[!t]

        \begin{center}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_T1.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_Truth.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_ClassicFCM.eps}}\\
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_RFCM.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_NLFCM.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_NLReg.eps}}
	\subfigure[]{\includegraphics[height=41mm, angle=180]{eps/chapitre3/IBSR_11_NLAll.eps}}
        \end{center}

        \caption[Résultats de la segmentation du cas n\textdegree11 de la base IBSR]{\emph{Résultats de la segmentation du cas n\textdegree11 de la base IBSR. (a) Coupe d'une image T1. (b) Vérité-terrain. (c) Segmentation par FCM classique. (d) Segmentation par RFCM. (e) Segmentation par NL-FCM. (f) Segmentation par NL-Reg. (g) Segmentation par NL-R-FCM.}}

        \label{FIG:VIEW:IBSR}

\end{figure}

\begin{figure}[!thbp]

        \begin{center}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/IBSR_GM.eps}}
	\subfigure[]{\includegraphics[height=38mm]{eps/chapitre3/IBSR_WM.eps}}
        \end{center}

        \caption[Coefficients Dice issus de la segmentation de la base IBSR]{\emph{Coefficients Dice issus de la segmentation de la base IBSR. (a) Matière grise. (b) Matière blanche. Légende : FCM : $+$, RFCM : $\diamond$, NL-Reg : $\times$, NL-FCM : $\circ$, NL-R-FCM : $\bigtriangledown$}}

        \label{FIG:DICE:IBSR}

\end{figure}

Le LCR n'est pas considéré dans cette étude car seuls les ventricules sont segmentés manuellement.
La moyenne de différentes segmentations à travers la base est calculée, ainsi que l'écart-type pour la matière blanche et la matière grise sont présentés au tableau~\ref{TAB:DICE:IBSR}.
Ces résultats montrent des performances similaires des différentes méthodologies concernant la segmentation de la matière blanche.
La méthode HMC \cite{Bricq:MIA:2008} semble plus consistante car elle obtient la meilleure moyenne absolue, mais également la plus faible dispersion.
Cependant, les méthodes fondées sur les champs et chaînes de Markov sont moins efficaces concernant la segmentation de la matière grise.
En effet, l'algorithme obtenant les meilleurs résultats pour ce tissu est R-FCM \cite{Pham:CVIU:2001}, et celui obtenant la plus faible dispersion est NL-R-FCM.
Les deux méthodologies obtenant les meilleurs scores globaux sont R-FCM et NL-R-FCM. 
Le premier obtient les meilleures moyennes et la plus faible dispersion globale.
Cependant, le deuxième obtient des résultats plus stables, illustré par une dispersion des scores équivalente pour la segmentation des deux tissus.

Les résultats pour chacun des cas individuels sont donnés par la figure~\ref{FIG:DICE:IBSR}.
Ils montrent que les différentes méthodes basées sur FCM obtiennent des résultats similaires.
Dans les quelques cas où des résultats significativement différents peuvent être observés, par exemple le cas n\textdegree4, les meilleurs résultats sont obtenus avec les méthodes considérant des centroïdes non stationnaires.
La raison de cette homogénéité globale des résultats peut être que la plupart des cas de la base ne nécessitent pas une correction du biais, ce dernier étant très peu présent.
Ceci peut expliquer que les méthodes NL-FCM et NL-R-FCM se comportent d'une manière non-optimale car elles sont dédiées à la correction de ce biais.
Néanmoins, nous pouvons signaler que la méthode NL-R-FCM, combinant donc régularisation et correction du biais, fournit les résultats d'ensemble les plus stables, ce qui est illustré par des écarts-types autour de $3.5$~\% aussi bien dans le cas de la matière blanche que de la matière grise.

Il faut cependant ajouter que ces résultats doivent être interprétés avec précaution.
En effet, les segmentations manuelles fournies avec la base IBSR ne sont pas toujours optimales (par exemple autour des ventricules ou bien dans des régions très circonvoluées tels que les sillons) comme illustré à la figure~\ref{FIG:VIEW:IBSR}.
L'analyse des résultats doit donc être complétée par une observation de la segmentation des différents cas.
Elle peut être révélatrice comme dans le cas n\textdegree11 où il apparaît clairement que le matière grise est sur-segmentée, constituant ainsi un sur-ensemble de la matière grise réelle, expliquant les scores relativement plus faibles dans ce cas que dans les autres.

\section{Conclusion}

Ce chapitre a présenté une nouvelle méthode de segmentation fondée sur FCM, intégrant les moyennes non-locales.
Nous avons développé un terme d'attache aux données et un terme de régularisation s'appuyant sur cette notion.
Le premier permet la segmentation d'une image présentant une inhomogénéité en intensité sans avoir besoin de l'évaluer explicitement.
Le second permet un lissage plus pertinent de la segmentation et accroît les performances de FCM dans un environnement bruité.

Les résutats obtenus par le terme de régularisation non-locale démontrent que cette méthodologie est toute indiquée si un bruit corrompt l'image à segmenter.
Cependant, ceux du terme d'attache aux données méritent un approfondissement et une exploration plus avancée des différentes options offertes par les moyennes non-locales.
L'utilisation d'un noyau à support compact peut par exemple être une alternative à tester afin d'éliminer l'influence des voxels peu similaires au voxel courant.

Le terme d'attache aux données repose sur le calcul de centroïdes locaux et sur le calcul des poids non-locaux permettant une pondération des modèles locaux environnants.
Ces deux ensembles de paramètres nécessitent la définition de deux larges voisinages, menant à des temps de calcul très élevés (ces deux zones de recherches ont une taille de : $17\times17\times17$).
En effet, environ huit heures de calcul sur un PC standard sont nécessaires pour segmenter une image (logiciel programmé en C++).
Différentes voies doivent être explorées afin de diminuer le temps d'exécution du logiciel.
Une solution pourrait être la décomposition en composante principales de l'ensemble des patches de l'image, ce qui aurait pour effet d'accélérer le traitement mais également d'améliorer le résultat de la segmentation~\cite{VanDeVille:TIP:2011}.
Une autre solution serait de décomposer l'image en sous-volumes disjoints, d'effectuer une évaluation locale du modèle dans ces sous-volume puis d'effectuer une interpolation des huit modèles les plus proches en chaque voxels pour avoir une estimation des centroïdes au voxel courant.
Le terme de régularisation ne nécessite au contraire que le calcul des poids non-locaux dans une zone de recherche plus réduite ($5\times5\times5$).
Les temps de calcul sont alors réduits à une dizaine de minutes.

Les expériences menées afin d'évaluer les deux termes de la fonction d'énergie de cette nouvelle version de FCM séparément ont montré qu'ils obtenaient de meilleurs résultats que les autres méthodologies testées.
Cependant, l'utilisation conjointe de ces termes non-locaux n'a pas apporté une amélioration du même ordre par rapport à ces méthodes dans un environnement à la fois biaisé et bruité.
Une étude plus poussée des interactions entre ces deux termes est nécessaire afin de comprendre la façon dont ils fonctionnent ensemble et permettre cette amélioration.

